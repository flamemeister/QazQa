{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flamemeister/QazQa/blob/main/Train_kazT5_base_30.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdS_l9_scsof",
        "outputId": "2b4c20bc-5ed8-4aa8-95b4-03e2476a1aa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.98\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import unicodedata\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import T5Tokenizer, T5TokenizerFast\n",
        "from transformers import T5ForConditionalGeneration\n",
        "from transformers import AdamW\n",
        "from transformers import BertForQuestionAnswering\n",
        "\n",
        "from tqdm import tqdm\n",
        "import sentencepiece\n",
        "\n",
        "from sklearn.model_selection import train_test_split \n",
        "from datetime import datetime\n",
        "\n",
        "max_text_length   = 256"
      ],
      "metadata": {
        "id": "Oz3Mp2NLc5q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = '/content/drive/MyDrive/НИРС/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srzd2xgRc7Si",
        "outputId": "04246dda-d282-4e0b-9060-1ec95ed6f23e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "    df = pd.read_csv(path, encoding='utf-16')\n",
        "    df = df.drop('Index', axis=1)\n",
        "    data = df.values.tolist()\n",
        "    data = [item[0] for item in data]\n",
        "    return data\n",
        "\n",
        "def to_utf8(text):\n",
        "    str(text).encode(encoding = 'UTF-16', errors = 'strict')\n",
        "    return text"
      ],
      "metadata": {
        "id": "bt3rEeHGdYh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 30000\n",
        "\n",
        "contexts  = load_data(path + 'Kaz_SQuAD/train_contexts_pdcsv.csv')[0:train_size]\n",
        "questions = load_data(path + 'Kaz_SQuAD/train_questions_pdcsv.csv')[0:train_size]\n",
        "answers   = load_data(path + 'Kaz_SQuAD/train_answers_pdcsv.csv')[0:train_size]"
      ],
      "metadata": {
        "id": "vVJSy0P4daHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_train, c_val, q_train, q_val, a_train, a_val = train_test_split(\n",
        "    contexts, questions, answers, \n",
        "    test_size=0.2,\n",
        "    random_state=73)"
      ],
      "metadata": {
        "id": "yPf_Y9UudbLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "model     = T5ForConditionalGeneration.from_pretrained(path + 'KazNLP/Tokenizers/t5-kaz-base')\n",
        "tokenizer = T5Tokenizer.from_pretrained(path+ 'KazNLP/Tokenizers/t5-kaz-base')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "KKwqcw7TdcZJ",
        "outputId": "df5fd5ec-fb63-4c16-8299-5e0a90db8443"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9dea58ee9dba>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'KazNLP/Tokenizers/t5-kaz-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'KazNLP/Tokenizers/t5-kaz-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'T5ForConditionalGeneration' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_token_positions(encodings, answers):\n",
        "    tokenized = tokenizer(answers, truncation=True, padding=True)\n",
        "    encodings.update({'target_ids': tokenized['input_ids'], 'target_attention_mask': tokenized['attention_mask']})\n",
        "\n",
        "train_encodings = tokenizer(c_train, q_train, max_length=max_text_length, truncation=True, padding=True)\n",
        "val_encodings   = tokenizer(c_val, q_val, max_length=max_text_length, truncation=True, padding=True)\n",
        "\n",
        "add_token_positions(train_encodings, a_train)\n",
        "add_token_positions(val_encodings, a_val)"
      ],
      "metadata": {
        "id": "rPzx4oXjdcd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "        print(encodings.keys())\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset = SquadDataset(train_encodings)\n",
        "val_dataset   = SquadDataset(val_encodings)\n",
        "train_loader  = DataLoader(train_dataset, batch_size=5, shuffle=True)"
      ],
      "metadata": {
        "id": "Cqa6lV_cdfL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "def save_model():\n",
        "    now = datetime.now()\n",
        "    date_time = now.strftime(\" %m %d %Y %H %M %S\")\n",
        "    torch.save(model.state_dict(), \"nlpModel\"+date_time+\".pt\")"
      ],
      "metadata": {
        "id": "3yO2u22ldhv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda') if cuda else torch.device('cpu')\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "vZmL4eiadiMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    for batch in loop:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # >\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        target_ids = batch['target_ids'].to(device)\n",
        "        target_attention_mask = batch['target_attention_mask'].to(device)\n",
        "        \n",
        "        outputs = model(input_ids, attention_mask=attention_mask,\n",
        "                        labels=target_ids,\n",
        "                        decoder_attention_mask=target_attention_mask)\n",
        "        # >\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loop.set_description(f'Epoch {epoch}')\n",
        "        loop.set_postfix(loss=loss.item())\n"
      ],
      "metadata": {
        "id": "eIGM2CbNdjWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda') if cuda else torch.device('cpu')\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "YoeyPRJddkgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(context, question):\n",
        "    input = tokenizer(context,\n",
        "                      question,\n",
        "                      max_length=max_text_length,\n",
        "                      truncation=True,\n",
        "                      padding=True)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        input_ids = torch.tensor(input['input_ids']).to(device)\n",
        "        attention_mask = torch.tensor(input['attention_mask']).to(device)\n",
        "        out = model.generate(input_ids,\n",
        "                             attention_mask=attention_mask,\n",
        "                             max_length=max_text_length,\n",
        "                             early_stopping=False)\n",
        "        return([tokenizer.decode(ids, skip_special_tokens=True) for ids in out[0]])"
      ],
      "metadata": {
        "id": "et58M1PFdklk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "story = [\"Алматыда оқушылар мен студенттер онлайн оқиды.\"]\n",
        "ques = [\"Алматыда кім оқиды?\"]\n",
        "\n",
        "print(test(story, ques))"
      ],
      "metadata": {
        "id": "85BbIVJLdnEo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}